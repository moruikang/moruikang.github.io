<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://moruikang.github.io</id>
    <title>Gridea</title>
    <updated>2021-01-19T14:32:40.775Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://moruikang.github.io"/>
    <link rel="self" href="https://moruikang.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://moruikang.github.io/images/avatar.png</logo>
    <icon>https://moruikang.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Gridea</rights>
    <entry>
        <title type="html"><![CDATA[一次OSS上传故障定位过程]]></title>
        <id>https://moruikang.github.io/post/yi-ci-oss-shang-chuan-gu-zhang-ding-wei-guo-cheng/</id>
        <link href="https://moruikang.github.io/post/yi-ci-oss-shang-chuan-gu-zhang-ding-wei-guo-cheng/">
        </link>
        <updated>2021-01-12T14:35:05.000Z</updated>
        <content type="html"><![CDATA[<h3 id="一-业务简介">一. 业务简介:</h3>
<p>前段时间接手了一个新系统，A站点是和设备交互的java springboot服务，提供websocket，http接口。处理设备请求，和下发控制指令到设备， 接受设备上传的文件请求，并把文件上传到OSS</p>
<p>###二. 问题现象描述</p>
<p>在生产环境，3次出现A站点偶发性在上传文件到阿里云OSS时出现异常，如下是A站点上传文件到OSS时抛出的异常:</p>
<pre><code>com.aliyun.oss.ClientException
	at com.aliyun.oss.common.utils.ExceptionFactory.createNetworkException(ExceptionFactory.java:71)
        ......

Caused by: java.net.SocketTimeoutException

...
java.net.SocketTimeoutException
at java.net.SocketInputStream.socketRead0(Native Method)
</code></pre>
<h4 id="1-确定影响范围">1)  确定影响范围</h4>
<p>A站点全线崩溃，从阿里云实时应用监控(ARMS)看到Java线程阻塞400多个； 业务上关键功能开闸指令不可用。</p>
<h4 id="2排查日志">2）排查日志</h4>
<p>时间节点基本处于业务高峰期，上传图片大部分失败，A站点接近崩溃，重启后过了一会又进入同样的状态。 第二次出现后还是没法定位到根本问题，A日志仅仅找到</p>
<p>[Client]Unable to execute HTTP request: SocketTimeout<br>
[Client]Unable to execute HTTP request: SocketTimeout<br>
[Client]Unable to execute HTTP request: SocketTimeout<br>
[Client]Unable to execute HTTP request: SocketTimeout</p>
<p>阿里云实时应用监控ARMS那边可以看到完整抛出的异常，一切异常都指向OSS java sdk；</p>
<pre><code>com.aliyun.oss.ClientException
	at com.aliyun.oss.common.utils.ExceptionFactory.createNetworkException(ExceptionFactory.java:71)
        ......

Caused by: java.net.SocketTimeoutException

...
java.net.SocketTimeoutException
at java.net.SocketInputStream.socketRead0(Native Method)
</code></pre>
<p>期间有调整过oss java sdk的相关参数，比如oss.maxConnections，oss.socketTimeout等。</p>
<h4 id="3-查看监控指标">3） 查看监控指标</h4>
<p>A站点pod的CPU,MEM 负载(load)一切正常，以及pod所在的服务器CPU，内存，负载，磁盘IO，网络(带宽未超出)一切正常。</p>
<h4 id="4临时方案">4）临时方案</h4>
<p>第二次出现这个状况后把上传OSS API接口分离出来，单独做一个A-upload站点，不影响A开闸业务；并且A-upload支持横向扩容，可在出现故障时横向pod扩容支撑业务正常运行。</p>
<h4 id="5怀疑方向">5）怀疑方向</h4>
<p>当时看异常错误，表象指向OSS服务端，或者上传OSS客户端（即代码端）</p>
<p>第三次出现OSS上传文件异常时，把A-upoad由2个pod横向扩展到4个pod，缓解业务压力，但旧的pod依然存在上传文件到OSS失败的问题。</p>
<p>从云监控看到OSS用户层级有效请求率(返回状态码是2xx,3xx)降低到0.<br>
<img src="https://moruikang.github.io/post-images/1611065016529.png" alt="" loading="lazy"></p>
<h3 id="三-定位和恢复">三.  定位和恢复</h3>
<p>第三次出现OSS异常时做了以下操作：</p>
<h4 id="1-临时恢复">1) 临时恢复</h4>
<p>横向扩展到4个pod后，删除原先2个异常的Pod</p>
<h4 id="2-保留现场">2) 保留现场</h4>
<p>把异常Pod的日志全部dump到本地<br>
登陆到某个异常的pod的节点，抓包分析本地主机和OSS TCP通信:</p>
<pre><code class="language-sudo"></code></pre>
<h4 id="3-缺憾">3) 缺憾</h4>
<p>未能保留完整的Pod现场，理论上可以新开deployment部署新的A-upload，修改Ingress把上传文件指向新的A-upload站点，保留旧的站点<br>
未能使用jstack在pod内部追踪java堆栈信息</p>
<h3 id="四-结果分析">四. 结果分析:</h3>
<h4 id="1通过wireshark分析抓包结果如图">1.通过Wireshark分析抓包结果如图：</h4>
<p>192.168.5.168是程序端主机ip， 106,14,228,23是OSS外网ip， TCP三次握手四次挥手过程出现大量TCP Retransmission，即网络丢包现象，请看如下相关知识解析<br>
<img src="https://moruikang.github.io/post-images/1611065280408.png" alt="" loading="lazy"></p>
<h5 id="tcp-retransmission">TCP Retransmission</h5>
<p>在网络环境中，数据包丢失是非常常见的事情。TCP协议有内建的处理机制来确保网络数据传输正确。但是，如果看到大量的TCP重传（TCP Retransmission），则往往表示存在异常。<br>
当数据发送方没有接收到接收方的ACK，就会TCP Retransmission。</p>
<p>虽然很多时候是由于网络链路问题，但是也有可能是服务器压力过大或者服务器端网卡、驱动等主机故障导致。所以，在监控中出现tcp retry告警时，不要轻易判断是网络故障，一定要对全链路进行分析。</p>
<p>我们设备调用A-upload站点Restful API上传OSS图片，A-uplad站点部署在阿里云VPC私网ECS k8s的pod内，和OSS enpoint同在一个内网，但是这里历史遗留原因使用了OSS外网地址；这台私网ECS通过一台具有公网IP的ECS主机做NAT路由转发和OSS通信，而此时服务器资源一切正常。</p>
<p>这里已经可以看出，A-upload站点通过NAT路由转发后和OSS外网地址上传图片会存在大量网络丢包现象。</p>
<h4 id="2-查看a-upload日志分析">2. 查看A-upload日志分析</h4>
<p>device-upload日志中发现:<br>
<img src="https://moruikang.github.io/post-images/1611065580049.png" alt="" loading="lazy"></p>
<p>存在大量处于 TIME_WAITING状态的线程，和ARMS看到的线程阻塞可以联系起来。</p>
<p>TIMED_WAITING: 线程在等待唤醒</p>
<p>如果发现大量线程处于此状态，并且从线程的堆栈上查看到是正在执行网络读写，这可能是一个网络瓶颈问题或者第三方响应慢的问题。</p>
<h3 id="总结">总结</h3>
<p>此次事故可以确定是在业务高峰时，A-upload使用OSS外网地址上传图片，A-upload站点所在ECS和OSS通信存在网络抖动，即丢包现象。</p>
<p>此外，我们对于线上问题定位不要被表面现象误导，以及被旁边同事不相干的怀疑误导，应该加强故障处理能力，加强工具建设：<br>
比如看到异常报错有：createNetworkException，就应该想到网络原因<br>
比如要学会使用Java常用诊断工具jstack，追踪堆栈调用等。</p>
]]></content>
    </entry>
</feed>